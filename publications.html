<!DOCTYPE html>
<html lang="en" data-theme="dark">
  <head>
    <!-- Font Awesome -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
  <!-- Academicons -->
  <link href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.2/css/academicons.min.css" rel="stylesheet">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Personal website of Abhiram Dodda, showcasing research, publications, and experience">
    <link rel="icon" type="image/png" sizes="32x32" href="data/img/ad.png">
    <meta name='keywords' content='portfolio, abhiram, dodda, abhiram dodda, portfolio website, personal website'>
    <meta name='author' content='Abhiram Dodda'>
    <title>Publications | Abhiram</title>
    
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" integrity="sha512-Fo3rlrZj/k7ujTTXRN7qzN1gM9A1g2A+z6GZL/RJ6EUt3phhFmvS/Jhtio0ZoEX2gB0ITh69S8VDuwvP/3vXcw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
      .hr {
            color: #E16A54;
        }
      .collapsible {
        cursor: pointer;
        text-align: left;
        outline: none;
      }
      
      .collapsible:after {
        color: white;
        font-weight: bold;
        float: right;
        margin-left: 5px;
      }
      
      .content {
        padding: 0 18px;
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.2s ease-out;
        background-color: var(--bg-color);
      }
      </style>
</head>
<body data-page="publications">
    <header>
        <nav class="nav-bar fixed-top">
          <div class="container nav-container">
            <button id="menu-toggle" aria-label="Toggle Menu" aria-expanded="false" class="toggle">
              <i class="fas fa-bars"></i>
            </button>
            <ul class="nav-links">
              <li><a href="index.html" class="nav-link" data-page="home">About</a></li>
              <li><a href="publications.html" class="nav-link active" data-page="publications">Publications</a></li>
              <li><a href="projects.html" class="nav-link" data-page="projects">Projects</a></li>
              <li><a href="experience.html" class="nav-link" data-page="experience">Experience</a></li>
              <li><a href="blog.html" class="nav-link" data-page="blog">Blog</a></li>
              <li><a href="activities.html" class="nav-link" data-page="activities">Activities</a></li>
              <li><a href="cv.html" class="nav-link" data-page="cv">CV</a></li>
            </ul>
            <button id="theme-toggle" aria-label="Toggle Theme">
              <i class="fas fa-sun"></i>
            </button>
          </div>
        </nav>
      </header>

  <main>
    <section id="publications">
        <div class="false-div-1"></div>
      <h1 class="about-h1">Publications</h1>

      <div>
        <table>
          <tr>
            <td>
              <!-- <img src="data/img/publication_1" alt="Publication 1" class="publication-image"/> -->
            </td>
            <td>
              <div class="publication-details">
                <h2>Ensemble Approach for Blood Vessel Segmentation in Retinal Images: Combining UNet and SegNet Models</h2>
                <p>Abhiram Dodda, Siddarth Mahesh B, Ekantha Sai Sundar Y, Kodali ANuradha, Shankar Babu B</p>
                <p>Conference: 4th International Conference on Intelligent Systems & Sustainable Computing (ICISSC-2024), Springer</p>
              </div>
            </td>
          </tr>
          <tr>
            <td></td>
            <td>
              <a href="https://www.researchgate.net/publication/386397886_Ensemble_Approach_for_Blood_Vessel_Segmentation_in_Retinal_Images_Combining_UNet_and_SegNet_Models" target="_blank"><button class="button-59" role="button">Paper</button></a>
              <a href="https://github.com/SiddharthMaheshB/retinal-image-segmentation" target="_blank"><button class="button-59" role="button">Code</button></a>
                <button class="button-59 collapsible" role="button">Abstract</button>
                <div class="content">
                  <p>It is critical for the proper segmentation of blood vessels in retinal images that for the early diagnosis and continuous monitoring of many retinal diseases, including diabetic retinopathy, glaucoma, and age-related macular degeneration. The paper introduces a novel approach that combines the strengths of architectures in deep learning with traditional machine learning techniques to boost performance in the segmentation of blood vessels in retinal images. Here, we combine the two well-known convolutional neural networks on the task of medical image segmentation, namely, U-Net and SegNet, with a logistic regression classifier at the end. Due to the symmetric architecture of the U-Net model, it catches complex vessel structures and fine details very well. The feature extraction capability of the SegNet model is good, and it uses the encoder-decoder framework. Using both models sequentially gives initial segmentation outputs. Fine tuning is performed with logistic regression in order to provide a more accurate and consistent output. Basically, it is a hybrid approach towards overcoming problems in blood vessel segmentation attributed to size variation of the vessel, contrast, and intensity levels in retinal images. Our approach was trained and tested on standard datasets of retinal images, and manifested the superiority of segmentation accuracy and the generalization ability. The results obtained with the final accuracy of 0.97021 outline the advantage in combining deep learning models with traditional machine learning classifiers to contribute not only to more precise segmentation but also to improved reliability of medical image analysis automated diagnostic systems, which opens further possibilities for real-world applications in retinal disease detection and management.</p>
                </div>
            </td>
          </tr>
        </table>
        
        <table>
          <tr>
            <td>
              <!-- <img src="data/img/publication_2.png" alt="Publication 1" class="publication-image"/> -->
            </td>
            <td>
              <div class="publication-details">
                <h2>Deep Learning and OCT Imaging: A Novel Ensemble Approach for Eye Disease Diagnosis</h2>
                <p>Abhiram Dodda, R Aruna Flarance, Kodali Anuradha, V Srilakshmi</p>
                <p>Conference: 4th International Conference on Cognitive & Intelligent Computing (ICCIC'24), Springer</p>
              </div>
            </td>
          </tr>
          <tr>
            <td></td>
            <td>
              <a href="https://www.researchgate.net/publication/386411316_Deep_Learning_and_OCT_Imaging_A_Novel_Ensemble_Approach_for_Eye_Disease_Diagnosis" target="_blank"><button class="button-59" role="button">Paper</button></a>
              <a href="https://github.com/AbhiramDodda/Eye-disease-using-OCT" target="_blank"><button class="button-59" role="button">Code</button></a>
                <button class="button-59 collapsible" role="button">Abstract</button>
                <div class="content">
                  <p>Most eye diseases, if not diagnosed, can result in severe vision impairments. The OCT scan images of the retina give a clear perception; hence, it is useful in diagnosing specific eye diseases. In this paper, the state-of-the-art deep learning technique is adopted with three CNN architectures: VGG16, Incep-tionV3, and InceptionResNetV2 for classifying diseases from OCT images. In this regard, ensemble methods tend to supplement the strength of each model and refine the classification accuracy. The high accuracy of 98.86% in the identification of ocular diseases has been improved with a marked improvement in models adapted to the OCT dataset, reusing existing knowledge.</p>
                </div>
            </td>
          </tr>
        </table>

        <table>
          <tr>
            <td>
              <!-- <img src="data/img/publication_3.png" alt="Publication 1" class="publication-image"/> -->
            </td>
            <td>
              <div class="publication-details">
                <h2>A Novel Deep-Learning Based Classification of Skin Diseases</h2>
                <p>Tabitha Indupalli, Abhiram Dodda, Rayapuraju Srivatsav, Singamsetty Aashrith, Vineeth Vudiga</p>
                <p>Conference: 5th IEEE Global Conference for Advancement in Technology (GCAT'24)</p>
              </div>
            </td>
          </tr>
          <tr>
            <td></td>
            <td>
              <a href="https://www.researchgate.net/publication/384760694_A_Novel_Deep-Learning_Based_Classification_of_Skin_Diseases" target="_blank"><button class="button-59" role="button">Paper</button></a>
              <a href="https://github.com/AbhiramDodda/SkinDiseaseDetection" target="_blank"><button class="button-59" role="button">Code</button></a>
              <a href="https://ieeexplore.ieee.org/abstract/document/10924047" target="_blank"><button class="button-59" role="button">IEEE</button></a>
                <button class="button-59 collapsible" role="button">Abstract</button>
                <div class="content">
                  <p>Skin diseases are caused by various factors including bacteria, fungus, and genetics. Some of the diseases causes rough, scaly patches on skin, while others, pose a serious health problems. Skin diseases if not treated at early stages can spread to other parts of body and complicate treatment. Using Deep Learning techniques, medical professionals can confirm presence of a disease and start treatment immediately. In this paper, Deep Convolutional Neural Network (CNN) was used to perform skin disease classification. Data acquired was imbalanced. Combination of Data Augmentation and Under-sampling was used to balance the dataset and prevent overfitting on majority classes. The Deep CNN model was able to achieve an accuracy of 95.27%. Thus Deep CNNs are able to classify skin disease images with sufficient accuracy.</p>
                </div>
            </td>
          </tr>
        </table>

        <table>
          <tr>
            <td>
              <!-- <img src="data/img/publication_4.jpeg" alt="Publication 1" class="publication-image"/> -->
            </td>
            <td>
              <div class="publication-details">
                <h2>A Novel Two-Stage Deep Learning Framework for Plant Disease Detection</h2>
                <p>Abhiram Dodda, Kodali Anuradha, V Srilakshmi, K Adilakshmi</p>
                <p>Conference: 5th IEEE Global Conference for Advancement in Technology (GCAT'24)</p>
              </div>
            </td>
          </tr>
          <tr>
            <td></td>
            <td>
              <a href="https://www.researchgate.net/publication/384760701_A_Novel_Two-Stage_Deep_Learning_Framework_for_Plant_Disease_Detection" target="_blank"><button class="button-59" role="button">Paper</button></a>
              <a href="https://github.com/AbhiramDodda/Two-stage-plant-diesease-detection" target="_blank"><button class="button-59" role="button">Code</button></a>
              <a href="https://ieeexplore.ieee.org/document/10924005" target="_blank"><button class="button-59" role="button">IEEE</button></a>
                <button class="button-59 collapsible" role="button">Abstract</button>
                <div class="content">
                  <p>This paper presents a two-stage image classification scheme for detecting plant leaf diseases using transfer learning techniques. Their timely detection and correct diagnosis are very important in applying crop management techniques that ensure food security. This model did better compared to the traditional single-stage classification models, since it achieved a high accuracy of 96.82%. The two stages are outstanding in focusing learning at each stage to attain superior accuracy for hierarchical classification. The first stage classifies plant species, and the second stage further classifies the disease. This paper presents an accurate and much more efficient method to undertake plant disease detection; it makes a great deal of difference in early disease intervention and crop yield optimization for sustainable agriculture practices.</p>
                </div>
            </td>
          </tr>
        </table>
        <hr class="hr">
        <h1 class="about-h1">Patents</h1>
        <table>
          <tr>
            <td>
              <div class="patent-img-div"></div>
            </td>
            <td>
              <div class="publication-details">
                <h2>Smart Glasses for Visually Impaired Individuals</h2>
                <p>Sreejyothsna Ankam, Manav, Abhiram Dodda, V Dinesh Chandra, Nagireddy Padmakshaya, P Deepthi</p>
                <p>Journal No. 02/2025, page no. 1229. Date of publication: 10/01/2025.</p>
              </div>
            </td>
          </tr>
          <tr>
            <td></td>
            <td>
              <a href="https://search.ipindia.gov.in/IPOJournal/Journal/Patent" target="_blank"><button class="button-59" role="button">HTML</button></a>
              <a href="data/patent_1.pdf" target="_blank"><button class="button-59" role="button">PDF</button></a>
                <button class="button-59 collapsible" role="button">Abstract</button>
                <div class="content">
                  <p>This system presents a shiny glass innovation that will help visually impaired people by analysing their surroundings through cameras and computer vision algorithms. 
                    The glasses provide real-time audio feedback, allowing users to move safely and confidently. They can identify objects, recognize obstacles, read written text using 
                    Optical Character Recognition (OCR), and distinguish currency for transactions. By fostering situational awareness, the device addresses challenges like navigation, 
                    reading, and safety hazards. Moreover, the glasses are connected to a mobile application where smartphone operation is possible via voice commands to access several 
                    features. The wearable gadget encourages independence by enabling its users to read, study, and move about in public or at home with full confidence. Future 
                    developments will aim at increasing the accuracy of object detection, hazard detection, and GPS integration for location tracking purposes.</p>
                </div>
            </td>
          </tr>
        </table>
      </div>
      <div class="false-div"></div>
    </section>
  </main>

  <footer class="footer">
    <div class="footer-content">
        <p>&copy; 2025 Abhiram Dodda. All rights reserved. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Last updated: August 28, 2025.</p>
    </div>
  </footer>
  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;
    
    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight){
          content.style.maxHeight = null;
        } else {
          content.style.maxHeight = content.scrollHeight + "px";
        } 
      });
    }
    </script>
  <script src="script.js"></script>
</body>
</html>